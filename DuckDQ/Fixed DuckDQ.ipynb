{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4336c2-040a-48b5-8842-5f2295496712",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
       "                                 Dload  Upload   Total   Spent    Left  Speed\n",
       "\n",
       "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
       "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
       "100 30301    0 30301    0     0  26099      0 --:--:--  0:00:01 --:--:-- 26076\n",
       "100  100k    0  100k    0     0  50585      0 --:--:--  0:00:02 --:--:-- 50560\n",
       "100  149k    0  149k    0     0  63362      0 --:--:--  0:00:02 --:--:-- 63362\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 30301    0 30301    0     0  26099      0 --:--:--  0:00:01 --:--:-- 26076\n100  100k    0  100k    0     0  50585      0 --:--:--  0:00:02 --:--:-- 50560\n100  149k    0  149k    0     0  63362      0 --:--:--  0:00:02 --:--:-- 63362\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "# clear the delta checkpoint\n",
    "rm -rf /dbfs/tmp/StreamingDataQuality/checkpoint\n",
    "\n",
    "# download some generated stock tick data; this is a public Mockaroo endpoint- as such, we can't guarantee availability!\n",
    "curl \"https://api.mockaroo.com/api/2aedaa80?count=1000&key=8eb06b50\" > /tmp/stockTicks.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98986e94-e4bd-4b26-9485-94646eda4d33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[2]: True</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[2]: True</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.mv(\"file:/tmp/stockTicks.json\", \"dbfs:/tmp/StreamingDataQuality/stockTicks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0097ae7d-205b-40b7-9381-a2f56c8c2d42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python interpreter will be restarted.\n",
       "Collecting git+https://github.com/tdoehmen/duckdq\n",
       "  Cloning https://github.com/tdoehmen/duckdq to /tmp/pip-req-build-jjfq78nz\n",
       "  Running command git clone -q https://github.com/tdoehmen/duckdq /tmp/pip-req-build-jjfq78nz\n",
       "Collecting duckdb&gt;=0.2.3\n",
       "  Downloading duckdb-0.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.1 MB)\n",
       "Collecting SQLAlchemy\n",
       "  Downloading SQLAlchemy-2.0.29-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
       "Requirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from duckdq==0.0.post0.dev9+g5032943) (1.2.4)\n",
       "Collecting whylabs-datasketches\n",
       "  Downloading whylabs_datasketches-2.2.0b1-cp38-cp38-manylinux2010_x86_64.whl (405 kB)\n",
       "Requirement already satisfied: dill in /databricks/python3/lib/python3.8/site-packages (from duckdq==0.0.post0.dev9+g5032943) (0.3.2)\n",
       "Collecting tryingsnake\n",
       "  Downloading tryingsnake-0.5.1.tar.gz (13 kB)\n",
       "Requirement already satisfied: pytz&gt;=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;duckdq==0.0.post0.dev9+g5032943) (2020.5)\n",
       "Requirement already satisfied: numpy&gt;=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;duckdq==0.0.post0.dev9+g5032943) (1.19.2)\n",
       "Requirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;duckdq==0.0.post0.dev9+g5032943) (2.8.1)\n",
       "Requirement already satisfied: six&gt;=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;duckdq==0.0.post0.dev9+g5032943) (1.15.0)\n",
       "Collecting typing-extensions&gt;=4.6.0\n",
       "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
       "Collecting greenlet!=0.4.17\n",
       "  Downloading greenlet-3.0.3-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (622 kB)\n",
       "Building wheels for collected packages: duckdq, tryingsnake\n",
       "  Building wheel for duckdq (setup.py): started\n",
       "  Building wheel for duckdq (setup.py): finished with status &#39;done&#39;\n",
       "  Created wheel for duckdq: filename=duckdq-0.0.post0.dev9+g5032943-py2.py3-none-any.whl size=43626 sha256=073ddc4b99c8c3900882b22382583d6b97aa4174ac9d8a65e6408e924caa483f\n",
       "  Stored in directory: /tmp/pip-ephem-wheel-cache-p5mrmawj/wheels/88/a5/74/5b722e499bbd67855336523f1b88d3078e69eeb50f563fbfce\n",
       "  Building wheel for tryingsnake (setup.py): started\n",
       "  Building wheel for tryingsnake (setup.py): finished with status &#39;done&#39;\n",
       "  Created wheel for tryingsnake: filename=tryingsnake-0.5.1-py3-none-any.whl size=11427 sha256=a8862ce1b7272d8adc3a9cb5d5864285025ec27ba4ee87df215c762099a2ef6a\n",
       "  Stored in directory: /root/.cache/pip/wheels/0a/e8/c5/7eda97b4220dd6b8cc007d52a6f41aae0e64b1d9a5572dfa28\n",
       "Successfully built duckdq tryingsnake\n",
       "Installing collected packages: typing-extensions, greenlet, whylabs-datasketches, tryingsnake, SQLAlchemy, duckdb, duckdq\n",
       "  Attempting uninstall: typing-extensions\n",
       "    Found existing installation: typing-extensions 3.7.4.3\n",
       "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9132a3ff-5d8b-46ff-8acb-198c4bf391a5\n",
       "    Can&#39;t uninstall &#39;typing-extensions&#39;. No files were found to uninstall.\n",
       "ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
       "tensorflow-cpu 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.11.0 which is incompatible.\n",
       "Successfully installed SQLAlchemy-2.0.29 duckdb-0.10.1 duckdq-0.0.post0.dev9+g5032943 greenlet-3.0.3 tryingsnake-0.5.1 typing-extensions-4.11.0 whylabs-datasketches-2.2.0b1\n",
       "WARNING: You are using pip version 21.0.1; however, version 24.0 is available.\n",
       "You should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-9132a3ff-5d8b-46ff-8acb-198c4bf391a5/bin/python -m pip install --upgrade pip&#39; command.\n",
       "Python interpreter will be restarted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting git+https://github.com/tdoehmen/duckdq\n  Cloning https://github.com/tdoehmen/duckdq to /tmp/pip-req-build-jjfq78nz\n  Running command git clone -q https://github.com/tdoehmen/duckdq /tmp/pip-req-build-jjfq78nz\nCollecting duckdb&gt;=0.2.3\n  Downloading duckdb-0.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.1 MB)\nCollecting SQLAlchemy\n  Downloading SQLAlchemy-2.0.29-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from duckdq==0.0.post0.dev9+g5032943) (1.2.4)\nCollecting whylabs-datasketches\n  Downloading whylabs_datasketches-2.2.0b1-cp38-cp38-manylinux2010_x86_64.whl (405 kB)\nRequirement already satisfied: dill in /databricks/python3/lib/python3.8/site-packages (from duckdq==0.0.post0.dev9+g5032943) (0.3.2)\nCollecting tryingsnake\n  Downloading tryingsnake-0.5.1.tar.gz (13 kB)\nRequirement already satisfied: pytz&gt;=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;duckdq==0.0.post0.dev9+g5032943) (2020.5)\nRequirement already satisfied: numpy&gt;=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;duckdq==0.0.post0.dev9+g5032943) (1.19.2)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;duckdq==0.0.post0.dev9+g5032943) (2.8.1)\nRequirement already satisfied: six&gt;=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;duckdq==0.0.post0.dev9+g5032943) (1.15.0)\nCollecting typing-extensions&gt;=4.6.0\n  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-3.0.3-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (622 kB)\nBuilding wheels for collected packages: duckdq, tryingsnake\n  Building wheel for duckdq (setup.py): started\n  Building wheel for duckdq (setup.py): finished with status &#39;done&#39;\n  Created wheel for duckdq: filename=duckdq-0.0.post0.dev9+g5032943-py2.py3-none-any.whl size=43626 sha256=073ddc4b99c8c3900882b22382583d6b97aa4174ac9d8a65e6408e924caa483f\n  Stored in directory: /tmp/pip-ephem-wheel-cache-p5mrmawj/wheels/88/a5/74/5b722e499bbd67855336523f1b88d3078e69eeb50f563fbfce\n  Building wheel for tryingsnake (setup.py): started\n  Building wheel for tryingsnake (setup.py): finished with status &#39;done&#39;\n  Created wheel for tryingsnake: filename=tryingsnake-0.5.1-py3-none-any.whl size=11427 sha256=a8862ce1b7272d8adc3a9cb5d5864285025ec27ba4ee87df215c762099a2ef6a\n  Stored in directory: /root/.cache/pip/wheels/0a/e8/c5/7eda97b4220dd6b8cc007d52a6f41aae0e64b1d9a5572dfa28\nSuccessfully built duckdq tryingsnake\nInstalling collected packages: typing-extensions, greenlet, whylabs-datasketches, tryingsnake, SQLAlchemy, duckdb, duckdq\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 3.7.4.3\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9132a3ff-5d8b-46ff-8acb-198c4bf391a5\n    Can&#39;t uninstall &#39;typing-extensions&#39;. No files were found to uninstall.\nERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-cpu 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.11.0 which is incompatible.\nSuccessfully installed SQLAlchemy-2.0.29 duckdb-0.10.1 duckdq-0.0.post0.dev9+g5032943 greenlet-3.0.3 tryingsnake-0.5.1 typing-extensions-4.11.0 whylabs-datasketches-2.2.0b1\nWARNING: You are using pip version 21.0.1; however, version 24.0 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-9132a3ff-5d8b-46ff-8acb-198c4bf391a5/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install git+https://github.com/tdoehmen/duckdq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800d1eb5-fefd-4a50-b2c9-3a4a7782e64c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from duckdq.checks import Check, CheckLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5328781-a833-40ba-bbcb-c585c12e9e20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">import spark.implicits._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.functions.concat\n",
       "data_path: String = /tmp/StreamingDataQuality/source/\n",
       "checkpoint_path: String = /tmp/StreamingDataQuality/checkpoint/\n",
       "base_df: org.apache.spark.sql.DataFrame = [buysell: string, date: string ... 6 more fields]\n",
       "empty_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [buysell: string, date: string ... 6 more fields]\n",
       "l1: Long = 0\n",
       "res0: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">import spark.implicits._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.concat\ndata_path: String = /tmp/StreamingDataQuality/source/\ncheckpoint_path: String = /tmp/StreamingDataQuality/checkpoint/\nbase_df: org.apache.spark.sql.DataFrame = [buysell: string, date: string ... 6 more fields]\nempty_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [buysell: string, date: string ... 6 more fields]\nl1: Long = 0\nres0: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.concat\n",
    "\n",
    "val data_path = \"/tmp/StreamingDataQuality/source/\"\n",
    "val checkpoint_path = \"/tmp/StreamingDataQuality/checkpoint/\"\n",
    "val base_df = spark.read.parquet(data_path)\n",
    "val empty_df = base_df.where(\"0 = 1\")\n",
    "val l1: Long = 0\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS trades_delta\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS bad_records\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS duck_metrics\")\n",
    "\n",
    "base_df.createOrReplaceTempView(\"trades_historical\")\n",
    "empty_df.write.format(\"delta\").saveAsTable(\"trades_delta\")\n",
    "empty_df.withColumn(\"batchID\",lit(l1)).write.format(\"delta\").saveAsTable(\"bad_records\")\n",
    "dbutils.fs.mkdirs(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d5f08a-6917-4d0f-a9a4-61acfc41bf88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from duckdq.checks import Check, CheckLevel\n",
    "from duckdq.verification_suite import VerificationSuite\n",
    "from duckdq.verification_suite import VerificationResult, VerificationSuite\n",
    "from duckdq.engines.state_engine import StateEngine\n",
    "from duckdq.utils.analysis_runner import AnalyzerContext\n",
    "from duckdq.utils.exceptions import StateMergingException\n",
    "from duckdq.verification_suite import VerificationResult, VerificationSuite\n",
    "import duckdb\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "\n",
    "df = spark.read.json(\"/tmp/StreamingDataQuality/stockTicks.json\")\n",
    "df = df.toPandas()\n",
    "\n",
    "verificationResult = (\n",
    "    VerificationSuite()\n",
    "    .on_data(df)\n",
    "    .add_check(\n",
    "    Check(CheckLevel.WARNING, \"Basic Check 2\")\n",
    "    .is_complete(\"ipaddr\")\n",
    "    .is_complete(\"quantity\")\n",
    "    .is_complete(\"price\")\n",
    "    .has_max(\"quantity\", lambda mx: mx <= 10000)\n",
    "    .is_non_negative(\"quantity\")\n",
    "        )\n",
    "        .run()\n",
    ")\n",
    "  \n",
    "data = []  # This will hold our extracted data\n",
    "\n",
    "for check, check_result in verificationResult.check_results.items():\n",
    "    check_description = check.description\n",
    "    for constraint_result in check_result.constraint_results:\n",
    "        constraint_s = constraint_result.constraint.__str__()\n",
    "        constraint_result_s = constraint_result.status.name\n",
    "        metric_value = constraint_result.metric.value\n",
    "        # Assuming `metric_value.get()` retrieves the actual value and is the standard success path\n",
    "        metric_s = metric_value.get() if metric_value.isSuccess else \"Error retrieving metric\"\n",
    "        # Append extracted information as a dictionary to our data list\n",
    "        data.append({\n",
    "            \"Check\": check_description,\n",
    "            \"Constraint\": constraint_s,\n",
    "            \"Status\": constraint_result_s,\n",
    "            \"Metric\": metric_s\n",
    "        })\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Check\", StringType(), True),\n",
    "    StructField(\"Constraint\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Metric\", StringType(), True)  # Use FloatType() or DoubleType() if applicable\n",
    "])\n",
    "\n",
    "\n",
    "ver_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Assuming you have a way to convert your metrics result to a DataFrame\n",
    "metric_results = ver_df # Your logic to convert analysis results to DataFrame\n",
    "metric_results.withColumn(\"ts\", current_timestamp()) \\\n",
    "    .write.format(\"delta\").mode(\"append\").saveAsTable(\"duck_metrics\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9621507f-995f-421a-9794-4d155f7d2a6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Check</th><th>Constraint</th><th>Status</th><th>Metric</th><th>ts</th></tr></thead><tbody><tr><td>Basic Check 2</td><td>CompletenessConstraint(Completeness(quantity))</td><td>FAILURE</td><td>0.952</td><td>2024-04-09T20:04:13.798+0000</td></tr><tr><td>Basic Check 2</td><td>CompletenessConstraint(Completeness(ipaddr))</td><td>FAILURE</td><td>0.965</td><td>2024-04-09T20:04:13.798+0000</td></tr><tr><td>Basic Check 2</td><td>CompletenessConstraint(Completeness(price))</td><td>FAILURE</td><td>0.986</td><td>2024-04-09T20:04:13.798+0000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Basic Check 2",
         "CompletenessConstraint(Completeness(quantity))",
         "FAILURE",
         0.952,
         "2024-04-09T20:04:13.798+0000"
        ],
        [
         "Basic Check 2",
         "CompletenessConstraint(Completeness(ipaddr))",
         "FAILURE",
         0.965,
         "2024-04-09T20:04:13.798+0000"
        ],
        [
         "Basic Check 2",
         "CompletenessConstraint(Completeness(price))",
         "FAILURE",
         0.986,
         "2024-04-09T20:04:13.798+0000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Check",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Constraint",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Metric",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5zZXNzaW9uIGltcG9ydCBTcGFya1Nlc3Npb24KZnJvbSBweXNwYXJrLnNxbCBpbXBvcnQgUm93CmZyb20gcHlzcGFyay5zcWwuZnVuY3Rpb25zIGltcG9ydCBjdXJyZW50X3RpbWVzdGFtcAppbXBvcnQgcGFuZGFzIGFzIHBkCmZyb20gcHlzcGFyay5zcWwuZnVuY3Rpb25zIGltcG9ydCBjb2wKCmRmID0gc3BhcmsudGFibGUoImR1Y2tfbWV0cmljcyIpCmRmID0gZGYud2l0aENvbHVtbigiTWV0cmljIiwgY29sKCJNZXRyaWMiKS5jYXN0KERvdWJsZVR5cGUoKSkpCgpkaXNwbGF5KGRmKQoKIyBpbXBvcnQgbWF0cGxvdGxpYi5weXBsb3QgYXMgcGx0CiMgaW1wb3J0IG1hdHBsb3RsaWIuZGF0ZXMgYXMgbWRhdGVzCgojIHBsdC5maWd1cmUoZmlnc2l6ZT0oMTAsIDYpKQoKIyAjIFBsb3QgZWFjaCBtZXRyaWMKIyBwbHQucGxvdChwZGZfYVsndHMnXSwgcGRmX2FbJ01ldHJpYyddLCBsYWJlbD0nTWV0cmljIEEnLCBtYXJrZXI9J28nLCBsaW5lc3R5bGU9Jy0nKQojIHBsdC5wbG90KHBkZl9iWyd0cyddLCBwZGZfYlsnTWV0cmljJ10sIGxhYmVsPSdNZXRyaWMgQicsIG1hcmtlcj0nbycsIGxpbmVzdHlsZT0nLScpCiMgcGx0LnBsb3QocGRmX2NbJ3RzJ10sIHBkZl9jWydNZXRyaWMnXSwgbGFiZWw9J01ldHJpYyBDJywgbWFya2VyPSdvJywgbGluZXN0eWxlPSctJykKCiMgIyBGb3JtYXR0aW5nIHRoZSBwbG90CiMgcGx0LnRpdGxlKCdNZXRyaWNzIE92ZXIgVGltZScpCiMgcGx0LnhsYWJlbCgnVGltZXN0YW1wJykKIyBwbHQueWxhYmVsKCdNZXRyaWMgVmFsdWUnKQojIHBsdC5sZWdlbmQoKQojIHBsdC54dGlja3Mocm90YXRpb249NDUpCiMgcGx0LmdjYSgpLnhheGlzLnNldF9tYWpvcl9mb3JtYXR0ZXIobWRhdGVzLkRhdGVGb3JtYXR0ZXIoJyVZLSVtLSVkICVIOiVNOiVTJykpCiMgcGx0LmdjYSgpLnhheGlzLnNldF9tYWpvcl9sb2NhdG9yKG1kYXRlcy5BdXRvRGF0ZUxvY2F0b3IoKSkKCiMgcGx0LnRpZ2h0X2xheW91dCgpICAjIEFkanVzdCBsYXlvdXQgdG8gbm90IGN1dCBvZmYgbGFiZWxzCiMgcGx0LnNob3coKQo=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView7eaecae\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView7eaecae\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView7eaecae\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView7eaecae) SELECT `ts`,SUM(`Metric`) `column_131b4d1732`,`Constraint` FROM q GROUP BY `Constraint`,`ts`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView7eaecae\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "Constraint",
             "id": "column_131b4d1734"
            },
            "y": [
             {
              "column": "Metric",
              "id": "column_131b4d1732",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_131b4d1732": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "94f3e6fc-3840-47e7-8dc8-126958ea6543",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Constraint",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "column_131b4d1732",
           "args": [
            {
             "column": "Metric",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "Constraint",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5zZXNzaW9uIGltcG9ydCBTcGFya1Nlc3Npb24KZnJvbSBweXNwYXJrLnNxbCBpbXBvcnQgUm93CmZyb20gcHlzcGFyay5zcWwuZnVuY3Rpb25zIGltcG9ydCBjdXJyZW50X3RpbWVzdGFtcAppbXBvcnQgcGFuZGFzIGFzIHBkCmZyb20gcHlzcGFyay5zcWwuZnVuY3Rpb25zIGltcG9ydCBjb2wKCmRmID0gc3BhcmsudGFibGUoImR1Y2tfbWV0cmljcyIpCmRmID0gZGYud2l0aENvbHVtbigiTWV0cmljIiwgY29sKCJNZXRyaWMiKS5jYXN0KERvdWJsZVR5cGUoKSkpCmRmX2ZpbHRlcmVkID0gZGYuZmlsdGVyKGNvbCgiQ29uc3RyYWludCIpLnN0YXJ0c3dpdGgoIkNvbXBsZXRlbmVzc0NvbnN0cmFpbnQiKSkKZGlzcGxheShkZl9maWx0ZXJlZCkK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView28135d7\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView28135d7\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView28135d7\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView28135d7) SELECT `Status`,SUM(`Metric`) `column_131b4d1753` FROM q GROUP BY `Status`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView28135d7\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 2",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "Status",
             "id": "column_131b4d1755"
            },
            "y": [
             {
              "column": "Metric",
              "id": "column_131b4d1753",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_131b4d1753": {
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "729bb8e6-3a4c-4f7a-aa02-4705565165c7",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 14.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Status",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Status",
           "type": "column"
          },
          {
           "alias": "column_131b4d1753",
           "args": [
            {
             "column": "Metric",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.table(\"duck_metrics\")\n",
    "df = df.withColumn(\"Metric\", col(\"Metric\").cast(DoubleType()))\n",
    "df_filtered = df.filter(col(\"Constraint\").startswith(\"CompletenessConstraint\"))\n",
    "display(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8560ef47-6c0f-4315-95a8-4ee96b375d31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2385787535382385,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Fixed 2024-04-08 21:04:21 (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
